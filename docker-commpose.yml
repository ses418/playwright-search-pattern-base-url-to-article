# ═══════════════════════════════════════════════════════════════════════════════
#  docker-compose.yml — runs BOTH services from the monorepo
#
#  Your service    → http://localhost:5060
#  Colleague's     → http://localhost:5061
#
#  Start everything:
#    docker-compose up --build
#
#  Start just your service:
#    docker-compose up --build scraper
#
#  Start just colleague's:
#    docker-compose up --build colleague
# ═══════════════════════════════════════════════════════════════════════════════

version: "3.9"

services:

  # ── YOUR scraper service ──────────────────────────────────────────────────
  scraper:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        SERVICE: scraper_service       # ← your folder name
    image: sesai-scraper:latest
    container_name: sesai-scraper
    restart: unless-stopped
    ports:
      - "5060:5060"
    env_file:
      - scraper_service/.env           # your .env (copy from .env.example)
    environment:
      # Can override .env values here if needed
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:5060/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      # Optional: mount for CSV backups / logs
      - scraper_data:/app/output

  # ── COLLEAGUE'S service ───────────────────────────────────────────────────
  colleague:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        SERVICE: colleague_service     # ← colleague's folder name
    image: sesai-colleague:latest
    container_name: sesai-colleague
    restart: unless-stopped
    ports:
      - "5070:5070"                    # maps their 5060 → host 5061
    env_file:
      - colleague_service/.env         # colleague's .env
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:5060/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      - colleague_data:/app/output

volumes:
  scraper_data:
  colleague_data: